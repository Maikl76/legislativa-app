import requests
import json
import os
import pandas as pd
from flask import Flask, render_template, request, jsonify, redirect, url_for
from bs4 import BeautifulSoup
import fitz  # PyMuPDF
from dotenv import load_dotenv

# Naƒçten√≠ environment√°ln√≠ch promƒõnn√Ωch
load_dotenv()
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

app = Flask(__name__)
app.secret_key = "supersecretkey"

# Cesty pro soubory
SOURCES_FILE = "sources.txt"
LEGAL_TEXTS_FILE = "legal_texts.txt"

# Inicializace datab√°ze
columns = ["N√°zev dokumentu", "Kategorie", "Datum vyd√°n√≠ / aktualizace", "Odkaz na zdroj", "Shrnut√≠ obsahu", "Soubor", "Kl√≠ƒçov√° slova", "P≈Øvodn√≠ obsah"]
legislativa_db = pd.DataFrame(columns=columns)

# ‚úÖ Naƒçteme seznam webov√Ωch zdroj≈Ø
def load_sources():
    if os.path.exists(SOURCES_FILE):
        with open(SOURCES_FILE, "r", encoding="utf-8") as file:
            return [line.strip() for line in file.readlines()]
    return []

# ‚úÖ P≈ôid√°me novou str√°nku do sources.txt
def save_source(url):
    with open(SOURCES_FILE, "a", encoding="utf-8") as file:
        file.write(url + "\n")

# ‚úÖ Naƒçteme ruƒçnƒõ p≈ôidan√© pr√°vn√≠ texty
def load_manual_legal_texts():
    if os.path.exists(LEGAL_TEXTS_FILE):
        with open(LEGAL_TEXTS_FILE, "r", encoding="utf-8") as file:
            return file.read()
    return ""

# ‚úÖ St√°hneme PDF dokument a extrahujeme text
def extract_text_from_pdf(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            pdf_document = fitz.open(stream=response.content, filetype="pdf")
            return "\n".join([page.get_text("text") for page in pdf_document]).strip()
    except Exception as e:
        print("Chyba p≈ôi zpracov√°n√≠ PDF:", e)
    return ""

# ‚úÖ St√°hneme seznam pr√°vn√≠ch p≈ôedpis≈Ø z webu
def scrape_legislation(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        data = []
        for link in soup.find_all("a", href=True):
            href = link["href"]
            if href.endswith(".pdf"):
                name = link.text.strip()
                full_url = href if href.startswith("http") else url[:url.rfind("/")+1] + href
                text_content = extract_text_from_pdf(full_url)
                data.append([name, "Legislativa", "N/A", url, "", full_url, "p≈ôedpisy", text_content])
        return pd.DataFrame(data, columns=columns)
    return pd.DataFrame(columns=columns)

# ‚úÖ Naƒçteme legislativn√≠ dokumenty
def load_initial_data():
    global legislativa_db
    urls = load_sources()
    legislativa_db = pd.concat([scrape_legislation(url) for url in urls], ignore_index=True)

load_initial_data()  # üÜï Naƒçteme dokumenty p≈ôi startu aplikace

# ‚úÖ API endpoint pro p≈ôid√°n√≠ nov√©ho webu
@app.route('/add_source', methods=['POST'])
def add_source():
    new_url = request.form.get("url").strip()
    if new_url:
        save_source(new_url)  # ‚úÖ Ulo≈æ√≠me URL do sources.txt
        load_initial_data()  # ‚úÖ Znovu naƒçteme v≈°echna data
    return redirect(url_for('index'))

# ‚úÖ Hlavn√≠ webov√° str√°nka
@app.route('/')
def index():
    sources = load_sources()
    return render_template('index.html', documents=legislativa_db.to_dict(orient="records"), sources=sources)

if __name__ == '__main__':
    app.run(debug=True)
