from flask import Flask, render_template, request, flash, jsonify
import os
import pandas as pd
import requests
from bs4 import BeautifulSoup
import fitz  # PyMuPDF
import threading
import time
import yagmail  # Bezpeƒçn√© odes√≠l√°n√≠ e-mail≈Ø
import difflib
from dotenv import load_dotenv

# Naƒçten√≠ environment√°ln√≠ch promƒõnn√Ωch
load_dotenv()

app = Flask(__name__)
app.secret_key = "supersecretkey"

# Cesty pro ukl√°d√°n√≠ dat
SOURCES_FILE = "sources.txt"
HISTORY_DIR = "historie_pdfs"

if not os.path.exists(HISTORY_DIR):
    os.makedirs(HISTORY_DIR)

# Naƒçten√≠ e-mailov√Ωch √∫daj≈Ø
EMAIL_ADDRESS = os.getenv("EMAIL_ADDRESS")
RECIPIENT_EMAIL = os.getenv("RECIPIENT_EMAIL")

# Inicializace datab√°ze
columns = ["N√°zev dokumentu", "Kategorie", "Datum vyd√°n√≠ / aktualizace", "Odkaz na zdroj", "Shrnut√≠ obsahu", "Soubor", "Kl√≠ƒçov√° slova", "P≈Øvodn√≠ obsah"]
legislativa_db = pd.DataFrame(columns=columns)
document_status = {}

def load_sources():
    if os.path.exists(SOURCES_FILE):
        with open(SOURCES_FILE, "r", encoding="utf-8") as file:
            return [line.strip() for line in file.readlines()]
    return []

def save_source(url):
    with open(SOURCES_FILE, "a", encoding="utf-8") as file:
        file.write(url + "\n")

def extract_text_from_pdf(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            pdf_document = fitz.open(stream=response.content, filetype="pdf")
            return "\n".join([page.get_text("text") for page in pdf_document]).strip()
    except Exception as e:
        print("Chyba p≈ôi zpracov√°n√≠ PDF:", e)
    return ""

def scrape_legislation(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        data = []
        for link in soup.find_all("a", href=True):
            href = link["href"]
            if href.endswith(".pdf"):
                name = link.text.strip()
                full_url = href if href.startswith("http") else url[:url.rfind("/")+1] + href
                text_content = extract_text_from_pdf(full_url)
                data.append([name, "Legislativa", "N/A", url, "", full_url, "p≈ôedpisy", text_content])
        return pd.DataFrame(data, columns=columns)
    return pd.DataFrame(columns=columns)

def update_legislation():
    global legislativa_db, document_status
    urls = load_sources()
    new_data = pd.concat([scrape_legislation(url) for url in urls], ignore_index=True)
    
    for index, row in new_data.iterrows():
        doc_name = row["N√°zev dokumentu"]
        new_text = row["P≈Øvodn√≠ obsah"]
        
        if doc_name not in legislativa_db["N√°zev dokumentu"].values:
            document_status[doc_name] = "Nov√Ω ‚úÖ"
        else:
            old_text = legislativa_db.loc[legislativa_db["N√°zev dokumentu"] == doc_name, "P≈Øvodn√≠ obsah"].values[0]
            if old_text != new_text:
                document_status[doc_name] = "Aktualizov√°no üü°"
                save_version(doc_name, old_text)
            else:
                document_status[doc_name] = "Beze zmƒõny ‚ö™"

    legislativa_db = new_data

@app.route('/')
def index():
    sources = load_sources()
    return render_template('index.html', documents=legislativa_db.to_dict(orient="records"), sources=sources, document_status=document_status)

@app.route('/search', methods=['POST'])
def search():
    query = request.form.get("query", "").strip().lower()
    results = []

    if not query:
        return jsonify({"error": "Zadejte hledan√Ω v√Ωraz!"})

    for _, doc in legislativa_db.iterrows():
        text = doc["P≈Øvodn√≠ obsah"]
        paragraphs = text.split("\n\n")
        for paragraph in paragraphs:
            if query in paragraph.lower():
                results.append({"text": paragraph.strip(), "document": doc["N√°zev dokumentu"], "source": doc["Odkaz na zdroj"]})

    return jsonify(results)

if __name__ == '__main__':
    thread = threading.Thread(target=update_legislation, daemon=True)
    thread.start()
    app.run(debug=True)
